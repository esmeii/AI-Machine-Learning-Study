{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PlantDiseaseDetection.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMRryNvcU8ZKtHGg+WC0vY9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hi-rama/AI-Machine-Learning-Study/blob/main/PlantDiseaseDetection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CiVD2CVes0C6"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this project, we are going to create an end-to-end Android application with TFLite. We opte to develop an Android application that detects plant diseases.\n",
        "\n",
        "The project is broken down into two steps:\n",
        "1. Building and creating a machine learning model using TensorFlow with Keras.\n",
        "2. Deploying the model to an Android application using TFLite. "
      ],
      "metadata": {
        "id": "Iftk_uRRs3hd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Machine Learning model using Tensorflow with Keras\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "lX27EX75tZ51"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We designed algorithms and models to reconize species and diseases in the crop leaves by using Convolutional Neural Network. We use Colab for edit source code."
      ],
      "metadata": {
        "id": "K0Rxv9lXte0j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing the Libraries"
      ],
      "metadata": {
        "id": "6z664NC6tsJ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install nightly package for some functionalirties that aren't in alpha\n",
        "!pip install tensorflow-gpu==2.0.0-betal\n",
        "# Install TF Hub for TF2\n",
        "!pip install 'tensorflow-hub == 0.5'\n",
        "\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import layers"
      ],
      "metadata": {
        "id": "OfN3WTaAtvo0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading the data"
      ],
      "metadata": {
        "id": "WbbDw6yrtuay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download a public dataset of 54,305 images of diseased and healthy plant leaves collected under controlled conditions PlantVillage Dataset. The images cover 14 species of crops, including: apple, blueberry, cherry, grape, orange, peach, pepper, potato, raspberry, soy, squash, strawberry and tomato. It contains images of 17 basic diseases, 4 bacterial diseases, 2 diseases caused by mold(oomycete), 2 viral diseases and 1 disease caused by a mite. 12 crop species also have healthy leaf images that are not visibly affected by disease."
      ],
      "metadata": {
        "id": "FVPVgki3u0s6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the training and validation directories"
      ],
      "metadata": {
        "id": "g0Bd_TWpviWg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Load data\n",
        "zip_fil=tf.keras.uils.get_file(origin='https://'fname='PlantVillage.zip',extract=True)\n",
        "#Create the training and validation directories\n",
        "data_dir = os.path.join(os.path.dirname(zip_file), 'PlantVillage')\n",
        "train_dir = os.path.join(data_dir,'train')\n",
        "validation_dir = os.path.join(data_dir,'validation')"
      ],
      "metadata": {
        "id": "X0b-v6a2vlvp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Label mapping"
      ],
      "metadata": {
        "id": "0KUk-LeUwMBp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You'll also need to load in a mapping from category label to category name. This will give you a dictionary mapping the integer encoded categories to the actual names of the plants and diseases."
      ],
      "metadata": {
        "id": "VP8VI0qEwNor"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/obeshor/Plant-diseases-Detector/archive/master.zip\n",
        "!unzip master.zip;\n",
        "import json\n",
        "with open('Plant-Diseases-Detector-master/categories.json','r') as f:\n",
        "  cat_to_name = json.load(f)\n",
        "  classes = list(cat_to_name.values())\n",
        "print(classes)"
      ],
      "metadata": {
        "id": "z3M2-8OIwoZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transfer Learning with TensorFlow Hub"
      ],
      "metadata": {
        "id": "Z3_qRQMlxHL5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Select the Hub/TF2 module to use, you have a choice with inception v3 or Mobilenet"
      ],
      "metadata": {
        "id": "q5_1dGC9xLlx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "module_selection = (\"inception_v3\", 299, 2048) #@param [\"(\\\"mobilenet_v2\\\", 224, 1280)\", \"(\\\"inception_v3\\\", 299, 2048)\"] {type:\"raw\", allow-input: true}\n",
        "handle_base,pixels,FV_SIZE = module_selection\n",
        "MODULE_HANDLE = \"https://tfhub.dev/google/tf2-preview/{}/feature_vector/2\".format(handle_base)\n",
        "IMAGE_SIZE = (pixels,pixels)\n",
        "BATCH_SIZE = 64 #@param{type:\"integer\"}"
      ],
      "metadata": {
        "id": "a2I9BPmmxWHd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Preprocessing"
      ],
      "metadata": {
        "id": "va6nQrHNyBsF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's set up data generators that will read pictures in our source folders, convert them to 'float32' tensors, and feed them to our network"
      ],
      "metadata": {
        "id": "ScfRsHipyEAv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you may already know, data that goes into neural networks should usually be normanized in some way to make it more amenable to processing by the network. In our case, we will preprocess our images by normalizing the pixel values to be in the '[0,1]' range (originally all values are in the '[0,255]' range). We'll need to make sure the input data is resized to 224x224 pixels or 299x299 pixels as required by the networks. You have the choice to implement image augmentation or not."
      ],
      "metadata": {
        "id": "0lCRH5hHyXqZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Input are suitably resized for the selected module.\n",
        "validation_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./225)\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "    validation_dir,\n",
        "    shuffle=False,\n",
        "    seed=42,\n",
        "    color_mode=\"rgb\"\n",
        "    class_mode=\"categorical\",\n",
        "    target_size=IMAGE_SIZE,\n",
        "    batch_size=BATCH_SIZE)\n",
        "do_data_augmentation = True #@param (type:\"boolean\")\n",
        "if do_data_augmentation:\n",
        "  train_datagen = tf.keras.preprocessing.imgae.ImageDataGenerator(\n",
        "      rescale=1./255,\n",
        "      rotation_range=40,\n",
        "      horizontal_flip=True,\n",
        "      width_shift_range=0.2,\n",
        "      height_shift_range=0.2,\n",
        "      shear_range=0.2,\n",
        "      zoom_range=0.2,\n",
        "      fill_mode='nearest'\n",
        "  )\n",
        "else:\n",
        "  train_datagen = validation_datagen\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    subset=\"training\",\n",
        "    shuffle=True,\n",
        "    seed=42,\n",
        "    color_mode=\"rgb\",\n",
        "    class_mode=\"categorical\",\n",
        "    target_size=IMAGE_SIZE,\n",
        "    batch_size=BATCH_SIZE\n",
        ")\n",
        ")"
      ],
      "metadata": {
        "id": "V9Vj9g9mzBL9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build the model"
      ],
      "metadata": {
        "id": "rVBRPmsM0tBT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "All it takes is to put a linear classifier on top of the feature_extractor with the Hub module. For speed, we start out with  a non-trainable feature_extractor, but you can also enable fine-tuning for greater accuracy but that taks a lot of time to train model"
      ],
      "metadata": {
        "id": "36CEHOGn0wn3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "featur_extractor = hub.KerasLayer(MODULE_HANDLE,\n",
        "                                  input_shape=IMAGE_SIZE+(3,),\n",
        "                                  output_shape=[FV_SIZE])\n",
        "do_fine_tuning = False #@param {type:\"boolean\"}\n",
        "if do_fine_tuning:\n",
        "  feature_extractor.trainable = True\n",
        "  # unfreeze some layers of base network for fine-tuning\n",
        "  for layer in feature_extractor.layers[-30]:\n",
        "    layer.trainable =True\n",
        "  else:\n",
        "    feature_extractor.tarinable = False\n",
        "model = tf.keras.Sequential([\n",
        "      feature_extractor,\n",
        "      tf.keras.layers.Flatten(),\n",
        "      tf.keras.layers.Dense(512,activation=\"relu\")\n",
        "      tf.keras.layers.Dropout(rate=0.2),\n",
        "      tf.keras.layers.Dense(train_generator.num_classes, activation='softmax',\n",
        "                            kernel_regularizer=tf.keras.regularizers.12(0.0001))\n",
        "])"
      ],
      "metadata": {
        "id": "iFXk0fzr1HyV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Specifying Loss Function and Optimizer"
      ],
      "metadata": {
        "id": "XqgMarDE2XX0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Compile model specifying the optimizer learning rate\n",
        "LEARNING_RATE = 0.001 #@param {type:\"number\"}\n",
        "model.compile(\n",
        "    optimizer=tf.keras.opimizers.Adam(lr=LEARNING_RATE),\n",
        "    loss='categorical_cressentropy',\n",
        "    metrics=['accuracy']\n",
        ")"
      ],
      "metadata": {
        "id": "bBwFUJwZ2cBO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training Model"
      ],
      "metadata": {
        "id": "u7ekcYG922MF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "train model using validation dataset for validate each steps. After 10 epochs, we get 94% for accuracy, you can improve this more than 99% using fine-tuning"
      ],
      "metadata": {
        "id": "eaSQRIxp231c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCES=10 #@param {type:\"integer\"}\n",
        "STEPS_EPOCHES= train_generator.samples//train_generator.batch_size\n",
        "VALID_STEPS=validation_generator.samples//validation_generator.batch_size\n",
        "histroy = model.fit_generator(\n",
        "    tran_generator,\n",
        "    steps_per_epoch = STEPS_EPOCHS,\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=VALID_STEPS\n",
        ")"
      ],
      "metadata": {
        "id": "EuRMtKxX3GVk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking Performance"
      ],
      "metadata": {
        "id": "xBKca0tD3tXs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot training and validation, accuracy and loss"
      ],
      "metadata": {
        "id": "v3E8OVrk3wo4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pylab as plt\n",
        "import numpy as np\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss= history.history['loss']\n",
        "val_loss=history.history['val_loss']\n",
        "epochs_range = range(EPOCHS)\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.subplt(1,2,1)\n",
        "plt.plot(epochs_range,acc,label='Trainig Accuracy')\n",
        "plt.plot(epochs_range,val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.ylabel(\"Accuracy (training and validation)\")\n",
        "plt.xlabel(\"Training Steps\")\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(epochs_range,loss,label='Training Loss')\n",
        "plt.plot(epochs_range,val_loss, label='Validaion Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and validation loss')\n",
        "plt.ylabel('loss(training and validation')\n",
        "plt.xlable(\"Training steps\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "tCtntKbt3z3H",
        "outputId": "839c860c-edd8-468e-abe4-9c38cd21fd37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-c5127aee4e0e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpylab\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random test"
      ],
      "metadata": {
        "id": "CmvtzVq65uQJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random five sample images from validation dataset and predict:"
      ],
      "metadata": {
        "id": "sK_guXAu5xSr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import OpenCV\n",
        "import cv2\n",
        "#Utility\n",
        "import itertools\n",
        "import random\n",
        "from collection import Counter\n",
        "from golb import iglob\n",
        "def load_image(filename):\n",
        "  img = cv2.imread(os.pah.join(data_dir, validation_dir, filename))\n",
        "  img = cv2.resize(img, (IMAGE_SIZE[0],IMAGE_SIZE[1]))\n",
        "  img = img/255\n",
        "  return img\n",
        "def predic(image):\n",
        "  probabilities = model.predict(np.asarray([img]))[0]\n",
        "  class_idx = np.argmax(probabilities)\n",
        "  return {classes[class_idx]: probabilities[class_idx]}\n",
        "for idx, filename in enumerate(random.sample(validation_generator.filename,5)):\n",
        "  print(\"SOURCE: class: %s, file: %s\" % (os.path.split(filename)[0],filename))\n",
        "  img = load_image(filename)\n",
        "  prediction = predict(img)\n",
        "  print(\"PREDICTED: class: %s, confidene: %f\" % (list(prediction.keys())[0], list(prediction.values())[0]))\n",
        "  plt.imshow(img)\n",
        "  plt.figure(idx)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "5mWQGEWi526G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model can be improved if you change some hyperparameters. You can try using a different pretrained model. It's up to you. Let me know if you can improve the accuracy!"
      ],
      "metadata": {
        "id": "YGLSDWxn7nZB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert model to TensorFlow Lite"
      ],
      "metadata": {
        "id": "u83AV6ed71KW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#convert the model to TFLite\n",
        "!mkdir \"tflite_models\"\n",
        "TFLITE_MODEL = \"tflite_models/plant_disease_model.tflite\"\n",
        "\n",
        "#Get the concrete function from the Keras model.\n",
        "run_model = tf.function(lambda x : reloaded(x))\n",
        "\n",
        "#Save the concrete function\n",
        "concrete_func = run_model.get_concreter_function(\n",
        "    tf.TensorSpec(model.inputs[0].shape, model.inputs[0].dtype)\n",
        ")\n",
        "#Convert the model to standard TensorFlow Lite model\n",
        "converter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])\n",
        "converted_tflite_model = converter.convert()\n",
        "open(TFLITE_MODEL, \"wb\").write(converted_tflite_model)\n"
      ],
      "metadata": {
        "id": "PU8H4Ogd75k4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add TFLite model in our Android Project"
      ],
      "metadata": {
        "id": "k2VxIyFP861k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First-load the model in our Androi project, we put plant_disease_model.tflite and plant_labels.txt into assets/ directory. plant_disease_model.tflite is the result of our previous colab notebook. We need to add TFLite dependency to app/build.gradle file.\n"
      ],
      "metadata": {
        "id": "uuepiRiX8-5E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "aaptOptions {\n",
        "    noCompress \"tflite\"\n",
        "}"
      ],
      "metadata": {
        "id": "RhdgQEUp9ZM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dive into the code"
      ],
      "metadata": {
        "id": "ntLiwPAb9etH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "create classifier class to load our model and read the file with labels:"
      ],
      "metadata": {
        "id": "rnTvTQr_9kRy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from io import BufferedReader\n",
        "from google.protobuf.descriptor import FileDescriptor\n",
        "from tensorflow.lite.python.interpreter import Interpreter\n",
        "class Classifier(assetManager: AssetManager, modelPath: String, labelPath: String, inputSize: int){\n",
        "    private var Interpreter: Interpreter\n",
        "    private var LABEL_LIST: List<String>\n",
        "    private val INPUT_SIZE: Int = inputSize\n",
        "    private val PIXEL_SIZE: Int = 3\n",
        "    private val IMAGE_MEAN = 0\n",
        "    private val IMAGE_STD = 255.0f\n",
        "    private val MAX_RESULTS = 3\n",
        "    private val THRESHOLD = 0.4f\n",
        "\n",
        "    ...\n",
        "    init{\n",
        "        INTERPRETER = Interpreter(loadModelFile(assetManager, modelPath))\n",
        "        LABEL_LIST = loadLabelList(assetManager,labelPath)\n",
        "    }\n",
        "    private fun loadModelFile(assetManager: AssetManager, modelPath: String):MappedByteBuffer{\n",
        "        val FileDescriptor = assetManager.openFd(modelPath)\n",
        "        val inputStream = FileInputStream(fileDescriptor.fileDescriptor)\n",
        "        val fileChannel = inputStream.channel\n",
        "        val startOffset = FileDescriptor.startOffset\n",
        "        val declaredLength = filDescriptor.declaredLength\n",
        "        return fileChannel.map(FileChannel.MapMode.READ_ONLY,startOffset,declaredLength)\n",
        "    }\n",
        "    private fun loadLabelList(assetManager: AssetManager, labelPath:String):List<String>{\n",
        "        return assetManager.open(labelPath).BufferedReader().useLines{it.toList()}\n",
        "    }\n",
        "...\n",
        "}"
      ],
      "metadata": {
        "id": "0BxLT3CG9qRR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Where Recognition is our humble result data class:"
      ],
      "metadata": {
        "id": "SRbQ-GR8_3gI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data class Recognition(\n",
        "        var id: String = \"\",\n",
        "        var title: String = \"\",\n",
        "        var confidence: Float = 0F\n",
        "    )  {\n",
        "        override fun toString(): String {\n",
        "            return \"Title = $title, Confidence = $confidence)\"\n",
        "        }\n",
        "    }"
      ],
      "metadata": {
        "id": "uFlJjXnh_8Yg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When we have an instance of Interpreter, we need to convert the preprocessed bitmap into ByteBuffer then we create a method that will take an image as an argument and return a list of labels with assigned probabilities to them:"
      ],
      "metadata": {
        "id": "f4CT0E1FAVaT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fun recognizeImage(bitmap: Bitmap): List<Classifier.Recognition>{\n",
        "    val scaledBitmap = Bitmap.createScaleBitmap(bitmap, INPUT_SIZE, INPUT_SIZE, false)\n",
        "    val byteBuffer = convertBitmapToByteBuffer(scaledBitmap)\n",
        "    val result = Array(1){FloatArray(LABEL_LIST.size)}\n",
        "    INTERPRETER.run(byteBuffer, result)\n",
        "    return getSortedResult(result)\n",
        "}"
      ],
      "metadata": {
        "id": "2c7zOYUgAsGO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's how we convert a bitmap into ByteBuffer:"
      ],
      "metadata": {
        "id": "F8mlrA9aBRvW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "private fun convertBitmapToByteBuffer(bitmap: Bitmap):ByteBuffer {\n",
        "    val byteBuffer = ByterBuffer.allocaeDirect(4*INPUT_SIZE*INPUT_SIZE*PIXEL_SIZE)\n",
        "    byteBuffer.order(ByteOrder.nativeOrder())\n",
        "    val intValues = IntArray(INPUT_SIZE*INPUT_SIZE)\n",
        "    bitmap.getPixels(intValues,0,bitmap.width,0,0,bitmap.width,bitmap.height)\n",
        "    var pixel = 0\n",
        "    for(i in 0 until INPUT_SIZE){\n",
        "        for(j in 0 until INPUT_SIZE)p\n",
        "        val 'val' = intValues[pixel++]\n",
        "        byteBuffer.putFloat(((('val'.shr(16) and 0xFF) - IMAGE_MEAN / IMAGE_STD))))\n",
        "        byteBuffer.putFloat(((('val'.shr(+) and 0xFF) - IMAGE_MEAN / IMAGE_STD))))\n",
        "\n",
        "        byteBuffer.putFloat(((('val'.shr(16) and 0xFF) - IMAGE_MEAN / IMAGE_STD))\n",
        "\n",
        "    }\n",
        "}\n",
        "return byteBuffer"
      ],
      "metadata": {
        "id": "pX9qDj18BWkR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}